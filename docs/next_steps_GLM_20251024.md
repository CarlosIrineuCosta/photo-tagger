# Project Enhancement Plan (2025-10-24)

This document provides a full assessment of the photo-tagger codebase and a plan to enhance its functionality towards a fully functional MVP.

## 1. Current State Assessment

The photo-tagger project is a powerful tool for AI-powered image tagging, with a comprehensive architecture that includes:
*   A Python backend with a FastAPI API.
*   A React-based frontend for user interaction.
*   A command-line interface (CLI) for batch operations.
*   Core logic for image processing, embedding generation (CLIP), and metadata handling (XMP).

The project is at an advanced stage of development, with many core features implemented. The current focus is on refining workflows, improving performance, and enhancing user experience to reach a stable MVP. The `docs/ongoing.md` file indicates a well-structured development process is already in place.

**Key Strengths:**
*   Modular architecture (frontend/backend/CLI).
*   Use of modern technologies (FastAPI, React).
*   Advanced features like medoid clustering and enhanced tagging are already being worked on.

**Areas for Enhancement (based on your request):**
*   **Workflow Clarity:** User workflows for tagging and filtering can be simplified and made more intuitive.
*   **Performance:** Initial data loading, thumbnail generation, and processing of large datasets are key bottlenecks.
*   **UI/UX:** The tags management page and gallery view can be improved for better scalability and usability.
*   **File Handling:** Robust handling of various file types (especially RAW and large files) is crucial.
*   **Deployment:** The system needs to be prepared for deployment on environments with different capabilities (e.g., no GPU).

## 2. Proposed Enhancements & Action Plan

Here is a point-by-point plan to address the issues and requirements you've outlined.

---

**(1-3). Core Functionality & States**

*   **User Goal**: Tag photos, handle new files, and manage different file states.
*   **Assessment**: The core logic seems to be in place but the states are causing confusion.
*   **Recommendation**: Formalize the file states into a clear state machine.

    *   **States Definition:**
        *   `UNPROCESSED`: New file, never seen by the system. No thumbnail, no tags.
        *   `PENDING_TAGS`: Thumbnail generated, but CLIP tagging has not been run.
        *   `UNSAVED_TAGS`: Tags generated by AI, but not yet reviewed or saved to XMP.
        *   `APPROVED`: User has reviewed and explicitly saved the tags to XMP.
        *   `MODIFIED`: A file with `APPROVED` tags has been modified (e.g. new tags added manually), and needs saving.

    *   **Action**: Refactor the backend logic to use these explicit states. This will simplify the filtering logic. The state can be stored in the application's database/store alongside the image data.

---

**(4). Filtering Logic**

*   **User Goal**: Clear and intuitive filters for the gallery.
*   **Assessment**: The current filters ('approved', 'only unapproved', 'hide saved') are confusing.
*   **Recommendation**: Replace the current filters with a simpler, state-based system.

    *   **Proposed Filters (UI):**
        *   A set of toggles or a dropdown menu:
            *   `Show: All`
            *   `Show: To Tag` (corresponds to `UNPROCESSED` + `PENDING_TAGS`)
            *   `Show: To Review` (corresponds to `UNSAVED_TAGS`)
            *   `Show: Approved` (corresponds to `APPROVED`)
            *   `Show: Modified` (corresponds to `MODIFIED`)

    *   **Implementation Snippet (Frontend - React):**
        ```typescript
        // In your gallery component state
        const [filter, setFilter] = useState('all'); // 'all', 'to_tag', 'to_review', 'approved'

        // Filter logic
        const filteredImages = images.filter(image => {
          if (filter === 'all') return true;
          if (filter === 'to_tag') return ['UNPROCESSED', 'PENDING_TAGS'].includes(image.state);
          if (filter === 'to_review') return image.state === 'UNSAVED_TAGS';
          if (filter === 'approved') return image.state === 'APPROVED';
          return false;
        });
        ```

---

**(5). New File Detection**

*   **User Goal**: Efficiently process only new files in a directory.
*   **Assessment**: This is a critical performance feature.
*   **Recommendation**: Enhance the scanning process to diff the directory against a known state.

    *   **Spec**:
        1.  On the first scan of a directory, store a list of all processed files and their modification times in a local cache file (e.g., `.tagger_cache.json`) in the scanned directory.
        2.  On subsequent scans, read this cache file.
        3.  Compare the files on disk with the cached list.
            *   Files present on disk but not in the cache are `NEW`.
            *   Files present in both but with a newer modification time are `MODIFIED`.
            *   Files in the cache but not on disk have been `DELETED`.
        4.  The scanner should then primarily focus on `NEW` and `MODIFIED` files.

    *   **Code Suggestion (`app/scanner.py` or similar):**
        ```python
        import os
        import json

        CACHE_FILE = '.tagger_cache.json'

        def get_directory_changes(directory):
            cache_path = os.path.join(directory, CACHE_FILE)
            
            # Load previous state
            try:
                with open(cache_path, 'r') as f:
                    last_state = json.load(f)
            except FileNotFoundError:
                last_state = {}

            # Get current state
            current_state = {
                f: os.path.getmtime(os.path.join(directory, f))
                for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))
            }

            new_files = [f for f in current_state if f not in last_state]
            modified_files = [f for f in current_state if f in last_state and current_state[f] > last_state[f]]
            
            # Update cache for next run
            with open(cache_path, 'w') as f:
                json.dump(current_state, f)

            return new_files, modified_files
        ```

---

**(6 & 15). Lazy Loading & Initial Fetch Performance**

*   **User Goal**: Implement lazy loading for large galleries and improve initial load time.
*   **Assessment**: Crucial for UX with large folders. `useIntersectionObserver` hook from `ongoing.md` is the right direction.
*   **Recommendation**: Implement backend pagination and a frontend infinite scroll component.

    *   **Backend API Spec (`/api/images` or similar):**
        *   Modify the endpoint to accept `page` and `page_size` query parameters.
        *   `GET /api/images?page=1&page_size=50`
        *   The endpoint should return a paginated response including the images and total count.
        ```json
        {
          "images": [...],
          "total": 10000,
          "page": 1,
          "has_more": true
        }
        ```

    *   **Frontend Implementation (`GalleryGrid.tsx`):**
        *   Use the `useIntersectionObserver` hook to detect when the user has scrolled to the bottom of the list.
        *   When the observer fires, fetch the next page of data from the API and append it to the existing list of images.
        *   Show a loading spinner at the bottom while fetching.

    *   **Code Suggestion (React):**
        ```typescript
        // In your gallery component
        const [images, setImages] = useState([]);
        const [page, setPage] = useState(1);
        const [hasMore, setHasMore] = useState(true);
        const loader = useRef(null);

        useEffect(() => {
            // Fetch initial data
            fetchImages(page);
        }, []);

        const fetchImages = (pageNum) => {
            // API call to fetch images for pageNum
            api.getImages({ page: pageNum }).then(response => {
                setImages(prev => [...prev, ...response.images]);
                setHasMore(response.has_more);
            });
        };

        // useIntersectionObserver setup
        const handleObserver = (entities) => {
            const target = entities[0];
            if (target.isIntersecting && hasMore) {
                setPage(p => p + 1);
            }
        };
        
        useEffect(() => {
            if(page > 1) fetchImages(page);
        }, [page]);

        // ... in render
        // <div ref={loader}>Loading...</div>
        ```
        This is a simplified example. The `useIntersectionObserver` hook from your codebase should be used.

---

**(7). File Type Handling (TIFFs)**

*   **User Goal**: Handle JPG, PNG, RAW, and show an error for very large TIFFs.
*   **Assessment**: Good practice to prevent memory issues.
*   **Recommendation**: Add a check in the file scanner.

    *   **Spec**:
        1.  In the file scanning logic, when a TIFF file is encountered (`.tif`, `.tiff`).
        2.  Check its file size.
        3.  If `file_size > 1GB` (or another configurable threshold), mark it as `UNSUPPORTED_LARGE` and do not attempt to process it.
        4.  The UI should display these files with an error icon and an explanation.

    *   **Code Suggestion (`app/scanner.py`):**
        ```python
        # Inside your file scanning loop
        file_path = os.path.join(directory, filename)
        if filename.lower().endswith(('.tif', '.tiff')):
            file_size_gb = os.path.getsize(file_path) / (1024**3)
            if file_size_gb > 1.0:
                # Log this file as unsupported
                unsupported_files.append({
                    "filename": filename,
                    "reason": "File size exceeds 1GB limit for TIFFs."
                })
                continue # Skip processing
        ```

---

**(8). Thumbnail Caching**

*   **User Goal**: A robust thumbnail caching system that runs first for new files.
*   **Assessment**: This is tied to the "New File Detection" (point 5).
*   **Recommendation**: Create a two-stage processing flow.

    *   **Spec**:
        1.  When a scan is initiated, first run the "New File Detection" logic (point 5).
        2.  For all `NEW` files, immediately start a background job to generate thumbnails.
        3.  The UI can show a message: "Detected X new files. Generating previews...".
        4.  The gallery can display placeholders for the images whose thumbnails are being generated, and update them as they become available. This provides immediate feedback to the user.
        5.  The main tagging process should only start after thumbnails are ready for a batch of images.

---

**(9). VPS / CPU-only Deployment**

*   **User Goal**: Prepare for deployment on a VPS without a GPU.
*   **Assessment**: This requires making the CLIP inference model configurable.
*   **Recommendation**:
    1.  **Configuration**: In `config.yaml`, add a `device` setting for the inference model: `device: 'cpu'`. The model loading code should use this config value.
    2.  **Performance Testing**: Create a CLI command or script to benchmark inference time on a CPU.
        ```bash
        python -m app.cli.tagger benchmark --device cpu --image_count 100
        ```
        This command would load the model onto the CPU and time the tagging of 100 sample images.
    3.  **Documentation**: Document the performance trade-offs in `docs/user_guide.md`. For CPU-only, tagging will be significantly slower, which reinforces the need for asynchronous processing.
    4.  **Asynchronous API**: As mentioned in `ongoing.md`, making `/api/process` asynchronous is a good long-term goal. For now, the UI must provide clear feedback that a long-running process is happening.

---

**(10). RAW File Pre-processing**

*   **User Goal**: Apply XMP sidecar adjustments before creating thumbnails.
*   **Assessment**: This is a complex but valuable feature for photographers. `rawpy` is a good library for this.
*   **Recommendation**: Investigate `rawpy`'s capabilities for XMP.

    *   **Spec**:
        1.  When processing a RAW file (e.g., `.CR2`, `.NEF`), check for an accompanying `.xmp` file.
        2.  Unfortunately, `rawpy` itself does not directly apply development settings from XMP files, as these are often proprietary to software like Adobe Lightroom. Applying them correctly is a very hard problem.
        3.  **Pragmatic Solution**: Use `rawpy`'s own auto-brightening feature as a good-enough proxy.
            ```python
            import rawpy
            import imageio

            with rawpy.imread(raw_path) as raw:
                # This will apply a basic curve to improve brightness
                rgb = raw.postprocess(use_camera_wb=True, no_auto_bright=False)
                # now save 'rgb' as a thumbnail
                # imageio.imsave(thumb_path, rgb)
            ```
        4.  **Alternative (Advanced)**: Explore using tools like `darktable-cli` to render the RAW with the XMP, but this adds a heavy external dependency. For an MVP, the `rawpy` auto-brightening is a more realistic start. I'd recommend starting with `rawpy`'s `postprocess` options.

---

**(11). Medoid Clarity**

*   **User Goal**: Understand the purpose of medoids and ensure the logic is sound.
*   **Assessment**: Medoids are used to find the most "representative" images in a cluster. This is great for quickly understanding a large set of similar photos.
*   **Explanation & Plan**:
    *   **Purpose**: In this context, medoids help you quickly review a large folder. Instead of looking at 1000 photos of a wedding, you can look at the 10 most representative photos (the medoids) to get the gist. If you approve the tags for a medoid, you could potentially apply those tags to all similar photos in its cluster.
    *   **Logic Check**: The current logic in `app/core/medoid.py` likely clusters images based on their CLIP embedding vectors. The medoid is the image closest to the center of a cluster.
    *   **Recommendation**:
        1.  **Heterogeneous Data**: For folders with varied content, the initial clustering is key. The number of clusters (`k`) should probably be determined dynamically (e.g., using the elbow method on cluster inertia) rather than being a fixed number.
        2.  **UI Integration**: The UI should visually group images by cluster, with the medoid for each cluster highlighted prominently. This is mentioned in `ongoing.md` ("Surface medoid cluster metadata across API + gallery UI"). This is the right path.
        3.  **Documentation**: Create a short guide in `docs/user_guide/medoids_workflow.md` explaining what medoids are and how to use them in the workflow. This is already a task in `ongoing.md`.

---

**(12). Ingestion Workflow**

*   **User Goal**: A well-thought-out ingestion workflow.
*   **Assessment**: This ties together points 5, 7, 8, and 10.
*   **Recommendation**: Consolidate the logic into a single, robust `IngestionService`.

    *   **Spec for `IngestionService`**:
        1.  **Input**: Directory path.
        2.  **Step 1: Scan & Diff**: Identify `new`, `modified`, `deleted` files (Point 5).
        3.  **Step 2: Pre-flight Checks**: For `new` files, check for unsupported types/sizes (Point 7).
        4.  **Step 3: Thumbnail Generation**: For `new` and `modified` files, generate thumbnails. Use `rawpy` for RAWs (Point 10), standard libraries for JPG/PNG. This should be a background process.
        5.  **Step 4: Update State**: The service updates the central data store with the status of all files (`UNPROCESSED`, `PENDING_TAGS`, etc.).
        6.  **Output**: A data structure that the frontend can consume to display the current state of the directory.

---

**(13). Future LLM Tag Processor**

*   **User Goal**: Prepare for a future LLM-based tag enhancement feature.
*   **Assessment**: Good foresight. A clear API contract is all that's needed for now.
*   **Recommendation**: Define a placeholder API endpoint.

    *   **API Spec (`backend/api/enhanced_tagging.py`):**
        ```python
        from fastapi import APIRouter
        from pydantic import BaseModel
        from typing import List

        router = APIRouter()

        class TagEnhancementRequest(BaseModel):
            tags: List[str]
            context: str # e.g., image filename or other metadata

        class TagEnhancementResponse(BaseModel):
            original_tags: List[str]
            enhanced_tags: List[str]
            explanation: str

        @router.post("/api/tags/enhance_with_llm", response_model=TagEnhancementResponse)
        async def enhance_tags_with_llm(request: TagEnhancementRequest):
            """
            (Future Implementation)
            This endpoint will take a list of tags and use a Large Language Model
            to refine them, cluster synonyms, and suggest better tags.
            For now, it returns a mock response.
            """
            # Mock implementation for now
            return TagEnhancementResponse(
                original_tags=request.tags,
                enhanced_tags=request.tags + ["llm-suggested-tag"],
                explanation="LLM processing is not yet implemented."
            )
        ```
    *   This creates the API contract. The frontend can be built against this, and the real LLM logic can be swapped in later without breaking changes.

---

**(14). Tags Page UI**

*   **User Goal**: A better UI for managing hundreds of tags.
*   **Assessment**: The "word pills" idea is excellent and common in modern UIs.
*   **Recommendation**: Implement a tag management component using "pills".

    *   **Spec**:
        *   Display all tags for an image as a list of "pills" (small, rounded rectangles with a label and an 'x' to remove).
        *   Have an input box to add new tags. As the user types, provide autocomplete suggestions from the existing tag corpus.
        *   For bulk operations, the `ongoing.md` plan for a "bulk promotion drawer" is the correct approach. This would allow selecting multiple tags (e.g., 'person', 'people') and merging them.
    *   **UI Component Library**: Look at existing React component libraries like `react-select` or Material-UI's `Chip` component, which provide this functionality out of the box. This will be faster than building it from scratch.

    *   **Code Suggestion (using a hypothetical `TagInput` component):**
        ```typescript
        // In your image detail view
        import TagInput from '.../components/TagInput';

        const [tags, setTags] = useState(['landscape', 'mountain', 'sunset']);
        const allKnownTags = ['landscape', ...]; // fetch from API

        const handleTagsChange = (newTags) => {
            setTags(newTags);
            // call API to save the new tags
        };

        return (
            <TagInput
                value={tags}
                onChange={handleTagsChange}
                autocompleteOptions={allKnownTags}
            />
        );
        ```

---

This plan provides a structured approach to tackling the outstanding issues and moving the project to a successful MVP. The next step would be to translate these specs into tickets or tasks in your project management system.
